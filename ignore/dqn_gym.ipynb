{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qqjdFQjb7mA"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from typing import NamedTuple, Sequence, Optional\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotnine as gg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical, Normal\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "class Transition(NamedTuple):\n",
        "  states: torch.Tensor\n",
        "  actions: torch.Tensor\n",
        "  log_probs: torch.Tensor\n",
        "  rewards: torch.Tensor\n",
        "  states_: torch.Tensor\n",
        "\n",
        "\n",
        "\n",
        "class Replay:\n",
        "\n",
        "  data: Optional[Sequence[np.ndarray]]\n",
        "  capacity: int\n",
        "  num_added: int\n",
        "\n",
        "  def __init__(self, capacity: int):\n",
        "    self.data = None\n",
        "    self.capacity = capacity\n",
        "    self.num_added = 0\n",
        "\n",
        "  def add(self, items: Sequence[Any]):\n",
        "    if self.data is None:\n",
        "      self.preallocate(items)\n",
        "\n",
        "    for slot, item in zip(self.data, items):\n",
        "      slot[self.num_added % self.capacity] = item\n",
        "\n",
        "    self.num_added += 1\n",
        "\n",
        "  def sample(self, size: int) -> Sequence[np.ndarray]:\n",
        "    indices = np.random.randint(self.size, size=size)\n",
        "    return [slot[indices] for slot in self.data]\n",
        "\n",
        "  def reset(self,):\n",
        "    self.data = None\n",
        "\n",
        "  @property\n",
        "  def size(self) -> int:\n",
        "    return min(self.capacity, self.num_added)\n",
        "\n",
        "  @property\n",
        "  def fraction_filled(self) -> float:\n",
        "    return self.size / self.capacity\n",
        "\n",
        "  def preallocate(self, items: Sequence[Any]):\n",
        "    as_array = []\n",
        "    for item in items:\n",
        "      as_array.append(np.asarray(item))\n",
        "\n",
        "    self.data = [np.zeros(dtype=x.dtype, shape=(self.capacity,) + x.shape)\n",
        "                  for x in as_array]\n",
        "\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self, obs_shape, hidden_size, num_actions):\n",
        "    self.linear = nn.Linear(np.prod(obs_shape), hidden_size)\n",
        "    self.q_head = nn.Linear(hidden_size, num_actions)\n",
        "    self.num_actions = num_actions\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x, start_dim=1)\n",
        "    x = F.relu(self.linear(x))\n",
        "    return self.q_head(x)\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "\n",
        "    clip_param = 0.2\n",
        "    max_grad_norm = 0.5\n",
        "    buffer_capacity = 1_000\n",
        "    batch_size = 32\n",
        "    discount = 0.99\n",
        "    target_network_update_interval = 100\n",
        "    epsilon = 0.01\n",
        "\n",
        "    def __init__(self, q_network, action_bound=None):\n",
        "        self.training_step = 0\n",
        "\n",
        "        self.q_network = q_network\n",
        "        self.target_network = copy.deepcopy(q_network)\n",
        "        self.replay = Replay(10_000)\n",
        "        self.counter = 0\n",
        "\n",
        "        self.optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # Epsilon greedy.\n",
        "        if np.random.rand() < self.epsilon:\n",
        "          return torch.randint(self.q_network.num_actions)\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.q_network(state)\n",
        "\n",
        "        return torch.argmax(q_values, axis=-1)\n",
        "\n",
        "    def update(self, transition):\n",
        "\n",
        "        self.replay.add(transition)\n",
        "        self.counter += 1\n",
        "        if self.counter % self.buffer_capacity != 0:\n",
        "          return  # Only do SGD step when we have a full batch.\n",
        "\n",
        "        self.training_step += 1\n",
        "\n",
        "        batch = self.replay.sample(self.batch_size)\n",
        "\n",
        "        s = torch.tensor([t.states for t in batch], dtype=torch.float)\n",
        "        a = torch.tensor([t.actions for t in batch], dtype=torch.float).view(-1, 1)\n",
        "        r = torch.tensor([t.rewards for t in batch], dtype=torch.float).view(-1, 1)\n",
        "        s_ = torch.tensor([t.states_ for t in batch], dtype=torch.float)\n",
        "\n",
        "        if self.training_step % self.target_network_update_interval == 0:\n",
        "          self.target_network = copy.deepcopy(self.q_network)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            target_q_values = self.q_network(s_)\n",
        "            target = r + self.discount * self.q_network(s_)\n",
        "\n",
        "            # TODO: Finalise DQN loss here.\n",
        "\n",
        "\n",
        "        target \n",
        "\n",
        "\n",
        "        del self.buffer[:]\n",
        "\n",
        "\n",
        "def main(num_episodes: int):\n",
        "\n",
        "    discrete = True  # TODO Fix\n",
        "    env = gym.make('CartPole-v1', new_step_api=True)\n",
        "    # env = gym.make('Pendulum-v1', new_step_api=True)\n",
        "\n",
        "    obs_shape = env.observation_space.shape\n",
        "\n",
        "    # actor = ContinuousActor(obs_shape, 100).float()\n",
        "    actor = DiscreteActor(obs_shape, 100, num_actions=env.action_space.n)\n",
        "    critic = Critic(obs_shape, 100).float()\n",
        "    agent = PPOAgent(actor, critic, action_bound=2)\n",
        "\n",
        "    results = []\n",
        "    state = env.reset()\n",
        "    for episode in range(num_episodes):\n",
        "        episode_return = 0\n",
        "        state = env.reset()\n",
        "\n",
        "        while True:\n",
        "            action, log_prob = agent.select_action(state)\n",
        "            env_action = action if discrete else [action]  # TODO: clean this up.\n",
        "            state_, reward, terminated, truncated, _ = env.step(env_action)\n",
        "\n",
        "            if terminated or truncated:\n",
        "              break\n",
        "\n",
        "            # TODO fix reward hack...\n",
        "            transition = Transition(state, action, log_prob, reward, state_)\n",
        "            agent.update(transition)\n",
        "            episode_return += reward\n",
        "            state = state_\n",
        "\n",
        "        result = {\n",
        "            'episode': episode,\n",
        "            'return': episode_return,  # TODO fix this.\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        if episode % 5 == 0:\n",
        "            print(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "results = main(1000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 ('envrl': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "696886948411669489bb36fbeb5884b73ac9ee24bc8c8d18a19fb4032db65ca7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
