{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Continuous Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11ca753f0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1')\n",
    "env.reset(seed=543)\n",
    "torch.manual_seed(543)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11ca753f0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset(seed=543)\n",
    "torch.manual_seed(543)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " everytime an action is selected , saved_log_probs gets updated \n",
    "1. we get the probability of actions from the inputting the state to the policy network, \n",
    "2. we get a categorical dist over the output from policy network\n",
    "3. action is a sample from the cat distribution \n",
    "4. we get the log probability of that action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "epsilon = 1e-6\n",
    "class Gaussian_Policy(nn.Module):\n",
    "    '''\n",
    "    Gaussian policy that consists of a neural network with 1 hidden layer that\n",
    "    outputs mean and log std dev (the params) of a gaussian policy\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_inputs, hidden_size, action_space):\n",
    "        super(Gaussian_Policy, self).__init__()\n",
    "\n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space.shape[0] # the number of output actions\n",
    "\n",
    "        self.linear = nn.Linear(num_inputs, hidden_size)\n",
    "        self.mean = nn.Linear(hidden_size, num_outputs)\n",
    "        self.log_std = nn.Linear(hidden_size, num_outputs)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=1e-2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # forward pass of NN\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear(x))\n",
    "\n",
    "        mean = self.mean(x)\n",
    "        log_std = self.log_std(x) # if more than one action this will give you the diagonal elements of a diagonal covariance matrix\n",
    "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX) # We limit the variance by forcing within a range of -2,20\n",
    "        std = log_std.exp()\n",
    "\n",
    "        return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For baseline in reinforce : V(s_t) = E[G_t | s_t] \"\"\"\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs,hidden_size):\n",
    "        super(ValueNetwork,self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=1e-2)\n",
    "\n",
    "    def forward(self,state):\n",
    "\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_policy = Gaussian_Policy(obs_dim, 128,env.action_space)\n",
    "# discrete_policy = Policy()\n",
    "optimizer = optim.Adam(continuous_policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "sigma = 0.2\n",
    "env_action = \"continuous\" # get from envs \n",
    "discount_factor = 0.99\n",
    "no_of_episodes = 10\n",
    "print_every = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_cont(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0) # just to make it a Tensor obj\n",
    "        # get mean and std\n",
    "    mean, std = continuous_policy(state)\n",
    "\n",
    "        # create normal distribution\n",
    "    normal = Normal(mean, std)\n",
    "        # sample action\n",
    "    action = normal.sample()\n",
    "\n",
    "        # get log prob of that action\n",
    "    ln_prob = normal.log_prob(action)\n",
    "    ln_prob = ln_prob.sum()\n",
    "\t# squeeze action into [-1,1]\n",
    "    action = torch.tanh(action)\n",
    "        # turn actions into numpy array\n",
    "    action = action.numpy()\n",
    "\n",
    "    return action[0], ln_prob \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assuming continuous here \n",
    "#Reinforce\n",
    "\n",
    "#main func:\n",
    "#inititae the env\n",
    "\n",
    "#initiate the policy network\n",
    "#initiate the value network\n",
    "\n",
    "#set the loop for certain number of exps\n",
    "\n",
    "#reset the env , and set an empty list of trajectories\n",
    "\n",
    "#collect a set of trajectories by letting the policy run in the env\n",
    "\n",
    "#train both policy and value functions:\n",
    "##remember : value funsction is mse between the value estimates and the return ; value estimates is just updating the network, feeding it the state \n",
    "##then to compute the advantage, we will minus rewards from the value function \n",
    "##policy training is as is ; but remember , the policy loss now uses advantage over simple return \n",
    "## accumulate policy loss and run gradient descent on it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "policy = Gaussian_Policy(obs_dim, 128,env.action_space)\n",
    "value = ValueNetwork(obs_dim,128)\n",
    "\n",
    "\n",
    "def train(data,use_baseline=True):\n",
    "    \"\"\"Args: data is the set of trajectories after running the policy in the env\"\"\"\n",
    "\n",
    "    rew = 0\n",
    "    rewards_togo = []\n",
    "    trajectory_rewards = [r for r in data[0]] #fix list comp\n",
    "    for i in trajectory_rewards[::-1]:\n",
    "        rew = i + gamma * rew\n",
    "        rewards_togo.insert(0,rew)\n",
    "\n",
    "    if use_baseline:\n",
    "        trajectory_observations = [obs[0] for obs in data] #check ordering \n",
    "        value_estimates = []\n",
    "        for observation in trajectory_observations:\n",
    "            obs = torch.from_numpy(obs).float().unsqueeze(0)\n",
    "            value_estimates.append(value(obs))\n",
    "\n",
    "        value_estimates = torch.stack(value_estimates).squeeze()\n",
    "\n",
    "        value_loss = F.mse_loss(value_estimates,rewards_togo)\n",
    "        value.optimizer.zero_grad\n",
    "        value_loss.backward()\n",
    "        value.optimizer.step()\n",
    "\n",
    "        adv_est = []\n",
    "        for trajectory_reward, val_est in zip(trajectory_rewards, value_estimates):\n",
    "            adv_est.append(trajectory_reward - val_est) #doube check is this reversed ??\n",
    "\n",
    "        log_probabilities = [lp[3] for lp in data] #check ordering \n",
    "\n",
    "        policy_loss = []\n",
    "        for lp,adv in zip(log_probabilities,value_estimates):\n",
    "            policy_loss.append(-lp * adv)\n",
    "\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        policy.optimizer.zero_grad()\n",
    "        policy.optimizer.step()\n",
    "\n",
    "        return policy_loss , value_loss\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_func(no_episodes,is_continuos = True):\n",
    "    if is_continuos:\n",
    "        env = gym.make('Pendulum-v1')\n",
    "        env.reset(seed=543)\n",
    "        torch.manual_seed(543)\n",
    "\n",
    "        policy = Gaussian_Policy(obs_dim, 128,env.action_space)\n",
    "        value = ValueNetwork(obs_dim,128) \n",
    "\n",
    "    for episode in range(no_of_episodes):\n",
    "        print('episode #', episode)\n",
    "        scores_deque = deque(maxlen=100)\n",
    "        trajectory_data = []\n",
    "        terminated = False\n",
    "  \n",
    "        state , info = env.reset()\n",
    "        for _ in range(500):\n",
    "            action , ln_prob = select_action_cont(state)\n",
    "            new_state,reward,terminated, truncated,_ = env.step(action)\n",
    "\n",
    "            trajectory_data.append([np.array(state),ln_prob,reward])\n",
    "            state = new_state\n",
    "\n",
    "            if terminated or truncated:\n",
    "\n",
    "                state, _ = env.reset()\n",
    "\n",
    "        value_loss , policy_loss = train(trajectory_data)\n",
    "\n",
    "        #go on to log value_loss and policy_loss\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode # 0\n",
      "Episode 0\tAverage Score: -4379.97\n",
      "episode # 1\n",
      "episode # 2\n",
      "Episode 2\tAverage Score: -4213.78\n",
      "episode # 3\n",
      "episode # 4\n",
      "Episode 4\tAverage Score: -4179.14\n",
      "episode # 5\n",
      "episode # 6\n",
      "Episode 6\tAverage Score: -3943.01\n",
      "episode # 7\n",
      "episode # 8\n",
      "Episode 8\tAverage Score: -4069.24\n",
      "episode # 9\n"
     ]
    }
   ],
   "source": [
    "#for continuous\n",
    "for episode in range(no_of_episodes):\n",
    "    print('episode #', episode)\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    log_probs = [] \n",
    "    rewards = []\n",
    "    terminated = False\n",
    "    episode_rewards = 0\n",
    "\n",
    "    state , info = env.reset()\n",
    "\n",
    "    for _ in range(500):\n",
    "        action , ln_prob = select_action_cont(state)\n",
    "        log_probs.append(ln_prob)\n",
    "        state,reward,terminated, truncated,info = env.step(action)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    scores_deque.append(sum(rewards))\n",
    "\n",
    "    cumulative = 0\n",
    "    discounted_r = np.zeros(len(rewards))\n",
    "    for rew in reversed(range(len(rewards))):\n",
    "        cumulative = cumulative * discount_factor + rewards[rew]\n",
    "        discounted_r[rew] = cumulative\n",
    "\n",
    "#normalize the discounted rewards\n",
    "    discounted_r -= np.mean(discounted_r)\n",
    "    discounted_r /= np.std(discounted_r)\n",
    "\n",
    "    loss = 0\n",
    "    for t in range(len(rewards)):\n",
    "\n",
    "        loss += -log_probs[t] * discounted_r[t] #loss is - log prob * total reward\n",
    "\n",
    "    continuous_policy.optimizer.zero_grad()\n",
    "    loss.backward() #update\n",
    "    continuous_policy.optimizer.step()\n",
    "\n",
    "    if episode % print_every == 0:\n",
    "       print('Episode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_deque)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8 % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal #continuous distribution\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self,lr):\n",
    "        super(Agent,self).__init__()\n",
    "        self.fc1 = nn.Linear(3,64)\n",
    "        self.fc2 = nn.Linear(64,32)\n",
    "        self.fc3 = nn.Linear(32,1) #neural network with layers 3,64,32,1\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.fc1(x)) #relu and tanh for output\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x)) * 2\n",
    "        return x\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "agent = Agent(0.01) #hyperparameters\n",
    "SIGMA = 0.2\n",
    "DISCOUNT = 0.99\n",
    "total = []\n",
    "\n",
    "for e in range(1000): \n",
    "    log_probs, rewards = [], []\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        mu = agent.forward(torch.from_numpy(state).float().unsq)\n",
    "        distribution = Normal(mu, SIGMA)\n",
    "        action = distribution.sample().clamp(-2.0,2.0)\n",
    "        log_probs.append(distribution.log_prob(action))\n",
    "        state, reward, done, info = env.step([action.item()])\n",
    "        #reward = abs(state[1])\n",
    "        rewards.append(reward)\n",
    "        \n",
    "    total.append(sum(rewards))\n",
    "\n",
    "    cumulative = 0\n",
    "    d_rewards = np.zeros(len(rewards))\n",
    "    for t in reversed(range(len(rewards))): #get discounted rewards\n",
    "        cumulative = cumulative * DISCOUNT + rewards[t]\n",
    "        d_rewards[t] = cumulative\n",
    "    d_rewards -= np.mean(d_rewards) #normalize\n",
    "    d_rewards /= np.std(d_rewards)\n",
    "\n",
    "    loss = 0\n",
    "    for t in range(len(rewards)):\n",
    "        loss += -log_probs[t] * d_rewards[t] #loss is - log prob * total reward\n",
    "\n",
    "    agent.optimizer.zero_grad()\n",
    "    loss.backward() #update\n",
    "    agent.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('envrl': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "696886948411669489bb36fbeb5884b73ac9ee24bc8c8d18a19fb4032db65ca7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
